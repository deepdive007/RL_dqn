{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from environment import Env\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 20\n",
    "\n",
    "\n",
    "# this is DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and replay memory & target q network\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.render = False\n",
    "\n",
    "        # actions which agent can do\n",
    "        self.action_space = [0,1,2,3]\n",
    "        # get size of state and action\n",
    "        self.action_size = len(self.action_space)\n",
    "        self.state_size = 22\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.epsilon = 1.  # exploration\n",
    "        self.epsilon_decay = .9999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 100\n",
    "\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        # copy the model to target model\n",
    "        # --> initialize the target model so that the parameters of model & target model to be same\n",
    "        self.update_target_model()\n",
    "\n",
    "\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(20, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        \n",
    "        \n",
    "     # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # The agent acts randomly\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            # Predict the reward value based on the given state\n",
    "            state = np.float32(state)\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])\n",
    "        \n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        \n",
    "        \n",
    "     # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            reward = np.float32(reward)\n",
    "            state = np.float32(state)\n",
    "            next_state = np.float32(next_state)\n",
    "            target = self.model.predict(state)[0]\n",
    "            \n",
    "            \n",
    "            # like Q Learning, get maximum Q value at s'\n",
    "            # But from target model\n",
    "            \n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target = reward + self.discount_factor * \\\n",
    "                                  np.amax(self.model.predict(next_state)[0])\n",
    "\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "            \n",
    "            \n",
    "        # make minibatch which includes target q value and predicted q value\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "    \n",
    "    \n",
    "    # load the saved model\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    # save the model which is under training\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 20)                460       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 964\n",
      "Trainable params: 964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 20)                460       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 964\n",
      "Trainable params: 964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n",
      "state = [0, 0, -2, -7, -3, -2, -2, -5, -4, -9, -5, -7, -6, -4, -7, -8, -8, -3, -9, -1, -9, -9]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-37ecb9976822>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"state =\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\sudeep_work\\RL_character_segmentation\\environment.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0morigin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUNIT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUNIT\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    # env = Maze()\n",
    "    env = Env()\n",
    "    agent = DQNAgent()\n",
    "    global_step = 0\n",
    "    scores, episodes = [], []\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        print(\"state =\",state)\n",
    "        state = np.reshape(state, [1, 22])\n",
    "    env.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Misc.grid_size of <environment.Env object .>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'coords': [125.0, 375.0], 'figure': 198, 'reward': -1, 'state': [2, 7]},\n",
       " {'coords': [175.0, 125.0], 'figure': 199, 'reward': -1, 'state': [3, 2]},\n",
       " {'coords': [125.0, 275.0], 'figure': 200, 'reward': -1, 'state': [2, 5]},\n",
       " {'coords': [225.0, 475.0], 'figure': 201, 'reward': -1, 'state': [4, 9]},\n",
       " {'coords': [275.0, 375.0], 'figure': 202, 'reward': -1, 'state': [5, 7]},\n",
       " {'coords': [325.0, 225.0], 'figure': 203, 'reward': -1, 'state': [6, 4]},\n",
       " {'coords': [375.0, 425.0], 'figure': 204, 'reward': -1, 'state': [7, 8]},\n",
       " {'coords': [425.0, 175.0], 'figure': 205, 'reward': -1, 'state': [8, 3]},\n",
       " {'coords': [475.0, 75.0], 'figure': 206, 'reward': -1, 'state': [9, 1]},\n",
       " {'coords': [475.0, 475.0], 'figure': 207, 'reward': 5, 'state': [9, 9]}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
